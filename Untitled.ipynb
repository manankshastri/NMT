{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 10000/10000 [00:00<00:00, 13735.98it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.09.70', '1970-09-10'),\n",
       " ('4/28/90', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('tuesday july 8 2008', '2008-07-08'),\n",
       " ('08 sep 1999', '1999-09-08'),\n",
       " ('1 jan 1981', '1981-01-01'),\n",
       " ('monday may 22 1995', '1995-05-22')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 9 may 1998\n",
      "Target date: 1998-05-09\n",
      "\n",
      "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: one_step_attention\n",
    "\n",
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: Define your pre-attention Bi-LSTM. Remember to use return_sequences=True. (≈ 1 line)\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n",
    "    model = Model([X, s0, c0], outputs)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 30, 37)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[1][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[2][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[3][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[4][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[5][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[6][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[7][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[8][0]            \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[9][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
      "                                                                 concatenate_1[1][0]              \n",
      "                                                                 concatenate_1[2][0]              \n",
      "                                                                 concatenate_1[3][0]              \n",
      "                                                                 concatenate_1[4][0]              \n",
      "                                                                 concatenate_1[5][0]              \n",
      "                                                                 concatenate_1[6][0]              \n",
      "                                                                 concatenate_1[7][0]              \n",
      "                                                                 concatenate_1[8][0]              \n",
      "                                                                 concatenate_1[9][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 30, 1)        11          dense_1[0][0]                    \n",
      "                                                                 dense_1[1][0]                    \n",
      "                                                                 dense_1[2][0]                    \n",
      "                                                                 dense_1[3][0]                    \n",
      "                                                                 dense_1[4][0]                    \n",
      "                                                                 dense_1[5][0]                    \n",
      "                                                                 dense_1[6][0]                    \n",
      "                                                                 dense_1[7][0]                    \n",
      "                                                                 dense_1[8][0]                    \n",
      "                                                                 dense_1[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 30, 1)        0           dense_2[0][0]                    \n",
      "                                                                 dense_2[1][0]                    \n",
      "                                                                 dense_2[2][0]                    \n",
      "                                                                 dense_2[3][0]                    \n",
      "                                                                 dense_2[4][0]                    \n",
      "                                                                 dense_2[5][0]                    \n",
      "                                                                 dense_2[6][0]                    \n",
      "                                                                 dense_2[7][0]                    \n",
      "                                                                 dense_2[8][0]                    \n",
      "                                                                 dense_2[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[1][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[2][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[3][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 attention_weights[4][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[5][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[6][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[7][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[8][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 attention_weights[9][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 64)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 dot_1[1][0]                      \n",
      "                                                                 lstm_1[0][0]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "                                                                 dot_1[2][0]                      \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[1][2]                     \n",
      "                                                                 dot_1[3][0]                      \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[2][2]                     \n",
      "                                                                 dot_1[4][0]                      \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[3][2]                     \n",
      "                                                                 dot_1[5][0]                      \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[4][2]                     \n",
      "                                                                 dot_1[6][0]                      \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[5][2]                     \n",
      "                                                                 dot_1[7][0]                      \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[6][2]                     \n",
      "                                                                 dot_1[8][0]                      \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[7][2]                     \n",
      "                                                                 dot_1[9][0]                      \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[8][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 11)           715         lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "                                                                 lstm_1[2][0]                     \n",
      "                                                                 lstm_1[3][0]                     \n",
      "                                                                 lstm_1[4][0]                     \n",
      "                                                                 lstm_1[5][0]                     \n",
      "                                                                 lstm_1[6][0]                     \n",
      "                                                                 lstm_1[7][0]                     \n",
      "                                                                 lstm_1[8][0]                     \n",
      "                                                                 lstm_1[9][0]                     \n",
      "==================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈2 lines)\n",
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2900/10000 [=======>......................] - ETA: 15:54 - loss: 23.9988 - dense_3_loss: 2.3967 - dense_3_acc: 0.0200 - dense_3_acc_1: 0.0000e+00 - dense_3_acc_2: 0.1200 - dense_3_acc_3: 0.1100 - dense_3_acc_4: 0.0000e+00 - dense_3_acc_5: 0.0000e+00 - dense_3_acc_6: 0.0300 - dense_3_acc_7: 0.0000e+00 - dense_3_acc_8: 0.0300 - dense_3_acc_9: 0.08 - ETA: 7:58 - loss: 23.8204 - dense_3_loss: 2.4065 - dense_3_acc: 0.0100 - dense_3_acc_1: 0.0700 - dense_3_acc_2: 0.0800 - dense_3_acc_3: 0.0650 - dense_3_acc_4: 0.4200 - dense_3_acc_5: 0.0350 - dense_3_acc_6: 0.0200 - dense_3_acc_7: 0.4700 - dense_3_acc_8: 0.0200 - dense_3_acc_9: 0.0450                 - ETA: 5:18 - loss: 23.6458 - dense_3_loss: 2.4209 - dense_3_acc: 0.0067 - dense_3_acc_1: 0.0467 - dense_3_acc_2: 0.0533 - dense_3_acc_3: 0.0433 - dense_3_acc_4: 0.6133 - dense_3_acc_5: 0.0233 - dense_3_acc_6: 0.0133 - dense_3_acc_7: 0.6467 - dense_3_acc_8: 0.0133 - dense_3_acc_9: 0.030 - ETA: 3:59 - loss: 23.4479 - dense_3_loss: 2.4502 - dense_3_acc: 0.0050 - dense_3_acc_1: 0.0350 - dense_3_acc_2: 0.0400 - dense_3_acc_3: 0.0325 - dense_3_acc_4: 0.7100 - dense_3_acc_5: 0.0175 - dense_3_acc_6: 0.0100 - dense_3_acc_7: 0.7350 - dense_3_acc_8: 0.0100 - dense_3_acc_9: 0.022 - ETA: 3:11 - loss: 23.2102 - dense_3_loss: 2.5045 - dense_3_acc: 0.0040 - dense_3_acc_1: 0.0280 - dense_3_acc_2: 0.0320 - dense_3_acc_3: 0.0260 - dense_3_acc_4: 0.7680 - dense_3_acc_5: 0.0140 - dense_3_acc_6: 0.0080 - dense_3_acc_7: 0.7880 - dense_3_acc_8: 0.0080 - dense_3_acc_9: 0.018 - ETA: 2:39 - loss: 23.0059 - dense_3_loss: 2.6052 - dense_3_acc: 0.0033 - dense_3_acc_1: 0.0233 - dense_3_acc_2: 0.0267 - dense_3_acc_3: 0.0217 - dense_3_acc_4: 0.8067 - dense_3_acc_5: 0.0117 - dense_3_acc_6: 0.0067 - dense_3_acc_7: 0.8233 - dense_3_acc_8: 0.0067 - dense_3_acc_9: 0.015 - ETA: 2:16 - loss: 22.8351 - dense_3_loss: 2.7052 - dense_3_acc: 0.0029 - dense_3_acc_1: 0.0200 - dense_3_acc_2: 0.0229 - dense_3_acc_3: 0.0186 - dense_3_acc_4: 0.8343 - dense_3_acc_5: 0.0100 - dense_3_acc_6: 0.0057 - dense_3_acc_7: 0.8486 - dense_3_acc_8: 0.0057 - dense_3_acc_9: 0.012 - ETA: 1:59 - loss: 22.6819 - dense_3_loss: 2.7528 - dense_3_acc: 0.0025 - dense_3_acc_1: 0.0638 - dense_3_acc_2: 0.0250 - dense_3_acc_3: 0.0162 - dense_3_acc_4: 0.8550 - dense_3_acc_5: 0.0088 - dense_3_acc_6: 0.0050 - dense_3_acc_7: 0.8675 - dense_3_acc_8: 0.0050 - dense_3_acc_9: 0.011 - ETA: 1:45 - loss: 22.5855 - dense_3_loss: 2.7847 - dense_3_acc: 0.0022 - dense_3_acc_1: 0.1022 - dense_3_acc_2: 0.0478 - dense_3_acc_3: 0.0200 - dense_3_acc_4: 0.8222 - dense_3_acc_5: 0.0322 - dense_3_acc_6: 0.0222 - dense_3_acc_7: 0.7711 - dense_3_acc_8: 0.0400 - dense_3_acc_9: 0.021 - ETA: 1:35 - loss: 22.4977 - dense_3_loss: 2.8055 - dense_3_acc: 0.0020 - dense_3_acc_1: 0.1300 - dense_3_acc_2: 0.0650 - dense_3_acc_3: 0.0270 - dense_3_acc_4: 0.7400 - dense_3_acc_5: 0.0500 - dense_3_acc_6: 0.0420 - dense_3_acc_7: 0.6940 - dense_3_acc_8: 0.0710 - dense_3_acc_9: 0.025 - ETA: 1:26 - loss: 22.4050 - dense_3_loss: 2.8093 - dense_3_acc: 0.0018 - dense_3_acc_1: 0.1555 - dense_3_acc_2: 0.0809 - dense_3_acc_3: 0.0318 - dense_3_acc_4: 0.7636 - dense_3_acc_5: 0.0509 - dense_3_acc_6: 0.0445 - dense_3_acc_7: 0.6409 - dense_3_acc_8: 0.0955 - dense_3_acc_9: 0.030 - ETA: 1:19 - loss: 22.3236 - dense_3_loss: 2.8079 - dense_3_acc: 0.0017 - dense_3_acc_1: 0.1733 - dense_3_acc_2: 0.0933 - dense_3_acc_3: 0.0325 - dense_3_acc_4: 0.7833 - dense_3_acc_5: 0.0467 - dense_3_acc_6: 0.0408 - dense_3_acc_7: 0.6708 - dense_3_acc_8: 0.0875 - dense_3_acc_9: 0.027 - ETA: 1:12 - loss: 22.2382 - dense_3_loss: 2.8074 - dense_3_acc: 0.0015 - dense_3_acc_1: 0.1846 - dense_3_acc_2: 0.0977 - dense_3_acc_3: 0.0308 - dense_3_acc_4: 0.8000 - dense_3_acc_5: 0.0431 - dense_3_acc_6: 0.0377 - dense_3_acc_7: 0.6962 - dense_3_acc_8: 0.0808 - dense_3_acc_9: 0.025 - ETA: 1:07 - loss: 22.1751 - dense_3_loss: 2.8089 - dense_3_acc: 0.0014 - dense_3_acc_1: 0.1979 - dense_3_acc_2: 0.1079 - dense_3_acc_3: 0.0286 - dense_3_acc_4: 0.8143 - dense_3_acc_5: 0.0400 - dense_3_acc_6: 0.0350 - dense_3_acc_7: 0.7179 - dense_3_acc_8: 0.0750 - dense_3_acc_9: 0.023 - ETA: 1:02 - loss: 22.1144 - dense_3_loss: 2.8050 - dense_3_acc: 0.0013 - dense_3_acc_1: 0.2087 - dense_3_acc_2: 0.1107 - dense_3_acc_3: 0.0267 - dense_3_acc_4: 0.8267 - dense_3_acc_5: 0.0373 - dense_3_acc_6: 0.0327 - dense_3_acc_7: 0.7367 - dense_3_acc_8: 0.0700 - dense_3_acc_9: 0.022 - ETA: 58s - loss: 22.0665 - dense_3_loss: 2.8092 - dense_3_acc: 0.0012 - dense_3_acc_1: 0.2144 - dense_3_acc_2: 0.1050 - dense_3_acc_3: 0.0250 - dense_3_acc_4: 0.8375 - dense_3_acc_5: 0.0350 - dense_3_acc_6: 0.0306 - dense_3_acc_7: 0.7531 - dense_3_acc_8: 0.0656 - dense_3_acc_9: 0.020 - ETA: 55s - loss: 22.0051 - dense_3_loss: 2.8073 - dense_3_acc: 0.0012 - dense_3_acc_1: 0.2276 - dense_3_acc_2: 0.0988 - dense_3_acc_3: 0.0235 - dense_3_acc_4: 0.8471 - dense_3_acc_5: 0.0329 - dense_3_acc_6: 0.0288 - dense_3_acc_7: 0.7676 - dense_3_acc_8: 0.0618 - dense_3_acc_9: 0.01 - ETA: 52s - loss: 21.9567 - dense_3_loss: 2.8188 - dense_3_acc: 0.0011 - dense_3_acc_1: 0.2361 - dense_3_acc_2: 0.1033 - dense_3_acc_3: 0.0222 - dense_3_acc_4: 0.8556 - dense_3_acc_5: 0.0311 - dense_3_acc_6: 0.0272 - dense_3_acc_7: 0.7806 - dense_3_acc_8: 0.0583 - dense_3_acc_9: 0.01 - ETA: 49s - loss: 21.9060 - dense_3_loss: 2.8186 - dense_3_acc: 0.0011 - dense_3_acc_1: 0.2442 - dense_3_acc_2: 0.1089 - dense_3_acc_3: 0.0211 - dense_3_acc_4: 0.8632 - dense_3_acc_5: 0.0295 - dense_3_acc_6: 0.0258 - dense_3_acc_7: 0.7921 - dense_3_acc_8: 0.0553 - dense_3_acc_9: 0.01 - ETA: 46s - loss: 21.8606 - dense_3_loss: 2.8191 - dense_3_acc: 1.0000e-03 - dense_3_acc_1: 0.2520 - dense_3_acc_2: 0.1095 - dense_3_acc_3: 0.0215 - dense_3_acc_4: 0.8700 - dense_3_acc_5: 0.0280 - dense_3_acc_6: 0.0245 - dense_3_acc_7: 0.8025 - dense_3_acc_8: 0.0525 - dense_3_acc_9: 0.01 - ETA: 44s - loss: 21.8116 - dense_3_loss: 2.8188 - dense_3_acc: 9.5238e-04 - dense_3_acc_1: 0.2576 - dense_3_acc_2: 0.1124 - dense_3_acc_3: 0.0233 - dense_3_acc_4: 0.8762 - dense_3_acc_5: 0.0267 - dense_3_acc_6: 0.0233 - dense_3_acc_7: 0.8119 - dense_3_acc_8: 0.0500 - dense_3_acc_9: 0.01 - ETA: 42s - loss: 21.7747 - dense_3_loss: 2.8186 - dense_3_acc: 9.0909e-04 - dense_3_acc_1: 0.2645 - dense_3_acc_2: 0.1159 - dense_3_acc_3: 0.0268 - dense_3_acc_4: 0.8818 - dense_3_acc_5: 0.0255 - dense_3_acc_6: 0.0223 - dense_3_acc_7: 0.8205 - dense_3_acc_8: 0.0477 - dense_3_acc_9: 0.01 - ETA: 40s - loss: 21.7180 - dense_3_loss: 2.8142 - dense_3_acc: 8.6957e-04 - dense_3_acc_1: 0.2748 - dense_3_acc_2: 0.1226 - dense_3_acc_3: 0.0317 - dense_3_acc_4: 0.8870 - dense_3_acc_5: 0.0243 - dense_3_acc_6: 0.0213 - dense_3_acc_7: 0.8283 - dense_3_acc_8: 0.0457 - dense_3_acc_9: 0.01 - ETA: 38s - loss: 21.6683 - dense_3_loss: 2.8150 - dense_3_acc: 8.3333e-04 - dense_3_acc_1: 0.2792 - dense_3_acc_2: 0.1267 - dense_3_acc_3: 0.0371 - dense_3_acc_4: 0.8717 - dense_3_acc_5: 0.0233 - dense_3_acc_6: 0.0204 - dense_3_acc_7: 0.8354 - dense_3_acc_8: 0.0437 - dense_3_acc_9: 0.01 - ETA: 36s - loss: 21.6138 - dense_3_loss: 2.8127 - dense_3_acc: 8.0000e-04 - dense_3_acc_1: 0.2836 - dense_3_acc_2: 0.1288 - dense_3_acc_3: 0.0412 - dense_3_acc_4: 0.8600 - dense_3_acc_5: 0.0224 - dense_3_acc_6: 0.0196 - dense_3_acc_7: 0.8420 - dense_3_acc_8: 0.0420 - dense_3_acc_9: 0.01 - ETA: 35s - loss: 21.5707 - dense_3_loss: 2.8081 - dense_3_acc: 7.6923e-04 - dense_3_acc_1: 0.2965 - dense_3_acc_2: 0.1327 - dense_3_acc_3: 0.0427 - dense_3_acc_4: 0.8654 - dense_3_acc_5: 0.0215 - dense_3_acc_6: 0.0188 - dense_3_acc_7: 0.8481 - dense_3_acc_8: 0.0404 - dense_3_acc_9: 0.01 - ETA: 33s - loss: 21.5264 - dense_3_loss: 2.8056 - dense_3_acc: 7.4074e-04 - dense_3_acc_1: 0.3096 - dense_3_acc_2: 0.1341 - dense_3_acc_3: 0.0444 - dense_3_acc_4: 0.8704 - dense_3_acc_5: 0.0207 - dense_3_acc_6: 0.0181 - dense_3_acc_7: 0.8537 - dense_3_acc_8: 0.0393 - dense_3_acc_9: 0.01 - ETA: 32s - loss: 21.4719 - dense_3_loss: 2.8018 - dense_3_acc: 7.1429e-04 - dense_3_acc_1: 0.3200 - dense_3_acc_2: 0.1379 - dense_3_acc_3: 0.0475 - dense_3_acc_4: 0.8750 - dense_3_acc_5: 0.0200 - dense_3_acc_6: 0.0175 - dense_3_acc_7: 0.8589 - dense_3_acc_8: 0.0411 - dense_3_acc_9: 0.01 - ETA: 31s - loss: 21.4254 - dense_3_loss: 2.7950 - dense_3_acc: 6.8966e-04 - dense_3_acc_1: 0.3290 - dense_3_acc_2: 0.1393 - dense_3_acc_3: 0.0483 - dense_3_acc_4: 0.8793 - dense_3_acc_5: 0.0193 - dense_3_acc_6: 0.0169 - dense_3_acc_7: 0.8638 - dense_3_acc_8: 0.0417 - dense_3_acc_9: 0.0234\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5800/10000 [================>.............] - ETA: 29s - loss: 21.3753 - dense_3_loss: 2.7876 - dense_3_acc: 6.6667e-04 - dense_3_acc_1: 0.3377 - dense_3_acc_2: 0.1413 - dense_3_acc_3: 0.0510 - dense_3_acc_4: 0.8833 - dense_3_acc_5: 0.0187 - dense_3_acc_6: 0.0163 - dense_3_acc_7: 0.8683 - dense_3_acc_8: 0.0463 - dense_3_acc_9: 0.02 - ETA: 28s - loss: 21.3242 - dense_3_loss: 2.7813 - dense_3_acc: 6.4516e-04 - dense_3_acc_1: 0.3458 - dense_3_acc_2: 0.1435 - dense_3_acc_3: 0.0523 - dense_3_acc_4: 0.8871 - dense_3_acc_5: 0.0181 - dense_3_acc_6: 0.0158 - dense_3_acc_7: 0.8726 - dense_3_acc_8: 0.0552 - dense_3_acc_9: 0.02 - ETA: 27s - loss: 21.2692 - dense_3_loss: 2.7786 - dense_3_acc: 6.2500e-04 - dense_3_acc_1: 0.3534 - dense_3_acc_2: 0.1475 - dense_3_acc_3: 0.0541 - dense_3_acc_4: 0.8906 - dense_3_acc_5: 0.0175 - dense_3_acc_6: 0.0153 - dense_3_acc_7: 0.8766 - dense_3_acc_8: 0.0553 - dense_3_acc_9: 0.03 - ETA: 26s - loss: 21.2132 - dense_3_loss: 2.7747 - dense_3_acc: 6.0606e-04 - dense_3_acc_1: 0.3606 - dense_3_acc_2: 0.1482 - dense_3_acc_3: 0.0558 - dense_3_acc_4: 0.8939 - dense_3_acc_5: 0.0170 - dense_3_acc_6: 0.0148 - dense_3_acc_7: 0.8803 - dense_3_acc_8: 0.0582 - dense_3_acc_9: 0.03 - ETA: 25s - loss: 21.1601 - dense_3_loss: 2.7713 - dense_3_acc: 5.8824e-04 - dense_3_acc_1: 0.3685 - dense_3_acc_2: 0.1503 - dense_3_acc_3: 0.0565 - dense_3_acc_4: 0.8971 - dense_3_acc_5: 0.0165 - dense_3_acc_6: 0.0144 - dense_3_acc_7: 0.8838 - dense_3_acc_8: 0.0635 - dense_3_acc_9: 0.03 - ETA: 24s - loss: 21.1118 - dense_3_loss: 2.7705 - dense_3_acc: 5.7143e-04 - dense_3_acc_1: 0.3766 - dense_3_acc_2: 0.1529 - dense_3_acc_3: 0.0577 - dense_3_acc_4: 0.9000 - dense_3_acc_5: 0.0160 - dense_3_acc_6: 0.0140 - dense_3_acc_7: 0.8871 - dense_3_acc_8: 0.0637 - dense_3_acc_9: 0.04 - ETA: 24s - loss: 21.0521 - dense_3_loss: 2.7644 - dense_3_acc: 5.5556e-04 - dense_3_acc_1: 0.3850 - dense_3_acc_2: 0.1550 - dense_3_acc_3: 0.0561 - dense_3_acc_4: 0.9028 - dense_3_acc_5: 0.0156 - dense_3_acc_6: 0.0136 - dense_3_acc_7: 0.8894 - dense_3_acc_8: 0.0719 - dense_3_acc_9: 0.04 - ETA: 23s - loss: 20.9999 - dense_3_loss: 2.7666 - dense_3_acc: 5.4054e-04 - dense_3_acc_1: 0.3919 - dense_3_acc_2: 0.1565 - dense_3_acc_3: 0.0546 - dense_3_acc_4: 0.9054 - dense_3_acc_5: 0.0151 - dense_3_acc_6: 0.0132 - dense_3_acc_7: 0.8911 - dense_3_acc_8: 0.0765 - dense_3_acc_9: 0.04 - ETA: 22s - loss: 20.9496 - dense_3_loss: 2.7633 - dense_3_acc: 5.2632e-04 - dense_3_acc_1: 0.3976 - dense_3_acc_2: 0.1568 - dense_3_acc_3: 0.0553 - dense_3_acc_4: 0.9079 - dense_3_acc_5: 0.0147 - dense_3_acc_6: 0.0129 - dense_3_acc_7: 0.8939 - dense_3_acc_8: 0.0745 - dense_3_acc_9: 0.04 - ETA: 21s - loss: 20.8951 - dense_3_loss: 2.7595 - dense_3_acc: 5.1282e-04 - dense_3_acc_1: 0.4023 - dense_3_acc_2: 0.1605 - dense_3_acc_3: 0.0538 - dense_3_acc_4: 0.9103 - dense_3_acc_5: 0.0144 - dense_3_acc_6: 0.0126 - dense_3_acc_7: 0.8954 - dense_3_acc_8: 0.0826 - dense_3_acc_9: 0.04 - ETA: 20s - loss: 20.8558 - dense_3_loss: 2.7637 - dense_3_acc: 5.0000e-04 - dense_3_acc_1: 0.4077 - dense_3_acc_2: 0.1653 - dense_3_acc_3: 0.0525 - dense_3_acc_4: 0.9125 - dense_3_acc_5: 0.0140 - dense_3_acc_6: 0.0122 - dense_3_acc_7: 0.8842 - dense_3_acc_8: 0.0885 - dense_3_acc_9: 0.04 - ETA: 20s - loss: 20.7968 - dense_3_loss: 2.7581 - dense_3_acc: 4.8780e-04 - dense_3_acc_1: 0.4137 - dense_3_acc_2: 0.1695 - dense_3_acc_3: 0.0544 - dense_3_acc_4: 0.9146 - dense_3_acc_5: 0.0137 - dense_3_acc_6: 0.0120 - dense_3_acc_7: 0.8871 - dense_3_acc_8: 0.0963 - dense_3_acc_9: 0.04 - ETA: 19s - loss: 20.7497 - dense_3_loss: 2.7566 - dense_3_acc: 0.0119 - dense_3_acc_1: 0.4183 - dense_3_acc_2: 0.1755 - dense_3_acc_3: 0.0543 - dense_3_acc_4: 0.9167 - dense_3_acc_5: 0.0133 - dense_3_acc_6: 0.0117 - dense_3_acc_7: 0.8895 - dense_3_acc_8: 0.0952 - dense_3_acc_9: 0.0502   - ETA: 18s - loss: 20.7069 - dense_3_loss: 2.7581 - dense_3_acc: 0.0265 - dense_3_acc_1: 0.4235 - dense_3_acc_2: 0.1795 - dense_3_acc_3: 0.0544 - dense_3_acc_4: 0.9186 - dense_3_acc_5: 0.0130 - dense_3_acc_6: 0.0114 - dense_3_acc_7: 0.8919 - dense_3_acc_8: 0.0937 - dense_3_acc_9: 0.05 - ETA: 18s - loss: 20.6522 - dense_3_loss: 2.7571 - dense_3_acc: 0.0409 - dense_3_acc_1: 0.4289 - dense_3_acc_2: 0.1834 - dense_3_acc_3: 0.0561 - dense_3_acc_4: 0.9205 - dense_3_acc_5: 0.0127 - dense_3_acc_6: 0.0111 - dense_3_acc_7: 0.8941 - dense_3_acc_8: 0.0955 - dense_3_acc_9: 0.05 - ETA: 17s - loss: 20.5986 - dense_3_loss: 2.7533 - dense_3_acc: 0.0542 - dense_3_acc_1: 0.4336 - dense_3_acc_2: 0.1860 - dense_3_acc_3: 0.0549 - dense_3_acc_4: 0.9222 - dense_3_acc_5: 0.0124 - dense_3_acc_6: 0.0109 - dense_3_acc_7: 0.8873 - dense_3_acc_8: 0.1020 - dense_3_acc_9: 0.05 - ETA: 17s - loss: 20.5517 - dense_3_loss: 2.7548 - dense_3_acc: 0.0663 - dense_3_acc_1: 0.4374 - dense_3_acc_2: 0.1900 - dense_3_acc_3: 0.0537 - dense_3_acc_4: 0.9239 - dense_3_acc_5: 0.0122 - dense_3_acc_6: 0.0107 - dense_3_acc_7: 0.8787 - dense_3_acc_8: 0.1061 - dense_3_acc_9: 0.05 - ETA: 16s - loss: 20.4988 - dense_3_loss: 2.7535 - dense_3_acc: 0.0785 - dense_3_acc_1: 0.4417 - dense_3_acc_2: 0.1923 - dense_3_acc_3: 0.0528 - dense_3_acc_4: 0.9255 - dense_3_acc_5: 0.0119 - dense_3_acc_6: 0.0104 - dense_3_acc_7: 0.8794 - dense_3_acc_8: 0.1102 - dense_3_acc_9: 0.05 - ETA: 16s - loss: 20.4444 - dense_3_loss: 2.7498 - dense_3_acc: 0.0900 - dense_3_acc_1: 0.4456 - dense_3_acc_2: 0.1958 - dense_3_acc_3: 0.0531 - dense_3_acc_4: 0.9271 - dense_3_acc_5: 0.0117 - dense_3_acc_6: 0.0102 - dense_3_acc_7: 0.8819 - dense_3_acc_8: 0.1117 - dense_3_acc_9: 0.05 - ETA: 15s - loss: 20.3925 - dense_3_loss: 2.7484 - dense_3_acc: 0.0982 - dense_3_acc_1: 0.4465 - dense_3_acc_2: 0.1986 - dense_3_acc_3: 0.0561 - dense_3_acc_4: 0.9286 - dense_3_acc_5: 0.0114 - dense_3_acc_6: 0.0100 - dense_3_acc_7: 0.8835 - dense_3_acc_8: 0.1155 - dense_3_acc_9: 0.06 - ETA: 14s - loss: 20.3351 - dense_3_loss: 2.7467 - dense_3_acc: 0.1090 - dense_3_acc_1: 0.4504 - dense_3_acc_2: 0.2030 - dense_3_acc_3: 0.0554 - dense_3_acc_4: 0.9300 - dense_3_acc_5: 0.0112 - dense_3_acc_6: 0.0098 - dense_3_acc_7: 0.8834 - dense_3_acc_8: 0.1212 - dense_3_acc_9: 0.06 - ETA: 14s - loss: 20.2780 - dense_3_loss: 2.7453 - dense_3_acc: 0.1182 - dense_3_acc_1: 0.4539 - dense_3_acc_2: 0.2063 - dense_3_acc_3: 0.0543 - dense_3_acc_4: 0.9314 - dense_3_acc_5: 0.0110 - dense_3_acc_6: 0.0096 - dense_3_acc_7: 0.8824 - dense_3_acc_8: 0.1271 - dense_3_acc_9: 0.06 - ETA: 14s - loss: 20.2199 - dense_3_loss: 2.7420 - dense_3_acc: 0.1285 - dense_3_acc_1: 0.4585 - dense_3_acc_2: 0.2096 - dense_3_acc_3: 0.0533 - dense_3_acc_4: 0.9327 - dense_3_acc_5: 0.0108 - dense_3_acc_6: 0.0094 - dense_3_acc_7: 0.8827 - dense_3_acc_8: 0.1313 - dense_3_acc_9: 0.06 - ETA: 13s - loss: 20.1617 - dense_3_loss: 2.7413 - dense_3_acc: 0.1383 - dense_3_acc_1: 0.4632 - dense_3_acc_2: 0.2106 - dense_3_acc_3: 0.0532 - dense_3_acc_4: 0.9336 - dense_3_acc_5: 0.0111 - dense_3_acc_6: 0.0092 - dense_3_acc_7: 0.8838 - dense_3_acc_8: 0.1345 - dense_3_acc_9: 0.06 - ETA: 13s - loss: 20.1073 - dense_3_loss: 2.7408 - dense_3_acc: 0.1478 - dense_3_acc_1: 0.4696 - dense_3_acc_2: 0.2126 - dense_3_acc_3: 0.0541 - dense_3_acc_4: 0.9330 - dense_3_acc_5: 0.0139 - dense_3_acc_6: 0.0094 - dense_3_acc_7: 0.8846 - dense_3_acc_8: 0.1352 - dense_3_acc_9: 0.06 - ETA: 12s - loss: 20.0521 - dense_3_loss: 2.7387 - dense_3_acc: 0.1562 - dense_3_acc_1: 0.4751 - dense_3_acc_2: 0.2149 - dense_3_acc_3: 0.0542 - dense_3_acc_4: 0.9335 - dense_3_acc_5: 0.0147 - dense_3_acc_6: 0.0093 - dense_3_acc_7: 0.8865 - dense_3_acc_8: 0.1345 - dense_3_acc_9: 0.06 - ETA: 12s - loss: 19.9949 - dense_3_loss: 2.7336 - dense_3_acc: 0.1643 - dense_3_acc_1: 0.4805 - dense_3_acc_2: 0.2186 - dense_3_acc_3: 0.0545 - dense_3_acc_4: 0.9337 - dense_3_acc_5: 0.0159 - dense_3_acc_6: 0.0091 - dense_3_acc_7: 0.8868 - dense_3_acc_8: 0.1364 - dense_3_acc_9: 0.07 - ETA: 11s - loss: 19.9332 - dense_3_loss: 2.7281 - dense_3_acc: 0.1711 - dense_3_acc_1: 0.4861 - dense_3_acc_2: 0.2225 - dense_3_acc_3: 0.0554 - dense_3_acc_4: 0.9340 - dense_3_acc_5: 0.0167 - dense_3_acc_6: 0.0089 - dense_3_acc_7: 0.8856 - dense_3_acc_8: 0.1411 - dense_3_acc_9: 0.07 - ETA: 11s - loss: 19.8767 - dense_3_loss: 2.7249 - dense_3_acc: 0.1786 - dense_3_acc_1: 0.4919 - dense_3_acc_2: 0.2248 - dense_3_acc_3: 0.0557 - dense_3_acc_4: 0.9340 - dense_3_acc_5: 0.0193 - dense_3_acc_6: 0.0088 - dense_3_acc_7: 0.8829 - dense_3_acc_8: 0.1462 - dense_3_acc_9: 0.0728"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8700/10000 [=========================>....] - ETA: 11s - loss: 19.8148 - dense_3_loss: 2.7220 - dense_3_acc: 0.1864 - dense_3_acc_1: 0.4973 - dense_3_acc_2: 0.2266 - dense_3_acc_3: 0.0569 - dense_3_acc_4: 0.9344 - dense_3_acc_5: 0.0210 - dense_3_acc_6: 0.0086 - dense_3_acc_7: 0.8841 - dense_3_acc_8: 0.1488 - dense_3_acc_9: 0.07 - ETA: 10s - loss: 19.7517 - dense_3_loss: 2.7180 - dense_3_acc: 0.1933 - dense_3_acc_1: 0.5022 - dense_3_acc_2: 0.2302 - dense_3_acc_3: 0.0572 - dense_3_acc_4: 0.9345 - dense_3_acc_5: 0.0255 - dense_3_acc_6: 0.0088 - dense_3_acc_7: 0.8850 - dense_3_acc_8: 0.1495 - dense_3_acc_9: 0.07 - ETA: 10s - loss: 19.6907 - dense_3_loss: 2.7152 - dense_3_acc: 0.2010 - dense_3_acc_1: 0.5084 - dense_3_acc_2: 0.2320 - dense_3_acc_3: 0.0579 - dense_3_acc_4: 0.9320 - dense_3_acc_5: 0.0374 - dense_3_acc_6: 0.0100 - dense_3_acc_7: 0.8823 - dense_3_acc_8: 0.1559 - dense_3_acc_9: 0.07 - ETA: 9s - loss: 19.6209 - dense_3_loss: 2.7128 - dense_3_acc: 0.2082 - dense_3_acc_1: 0.5145 - dense_3_acc_2: 0.2337 - dense_3_acc_3: 0.0577 - dense_3_acc_4: 0.9331 - dense_3_acc_5: 0.0458 - dense_3_acc_6: 0.0110 - dense_3_acc_7: 0.8834 - dense_3_acc_8: 0.1598 - dense_3_acc_9: 0.0761 - ETA: 9s - loss: 19.5596 - dense_3_loss: 2.7105 - dense_3_acc: 0.2151 - dense_3_acc_1: 0.5176 - dense_3_acc_2: 0.2346 - dense_3_acc_3: 0.0586 - dense_3_acc_4: 0.9341 - dense_3_acc_5: 0.0508 - dense_3_acc_6: 0.0114 - dense_3_acc_7: 0.8852 - dense_3_acc_8: 0.1613 - dense_3_acc_9: 0.076 - ETA: 9s - loss: 19.4915 - dense_3_loss: 2.7064 - dense_3_acc: 0.2223 - dense_3_acc_1: 0.5216 - dense_3_acc_2: 0.2355 - dense_3_acc_3: 0.0583 - dense_3_acc_4: 0.9352 - dense_3_acc_5: 0.0595 - dense_3_acc_6: 0.0112 - dense_3_acc_7: 0.8870 - dense_3_acc_8: 0.1644 - dense_3_acc_9: 0.077 - ETA: 8s - loss: 19.4237 - dense_3_loss: 2.7018 - dense_3_acc: 0.2300 - dense_3_acc_1: 0.5271 - dense_3_acc_2: 0.2368 - dense_3_acc_3: 0.0588 - dense_3_acc_4: 0.9362 - dense_3_acc_5: 0.0708 - dense_3_acc_6: 0.0118 - dense_3_acc_7: 0.8888 - dense_3_acc_8: 0.1674 - dense_3_acc_9: 0.078 - ETA: 8s - loss: 19.3531 - dense_3_loss: 2.6978 - dense_3_acc: 0.2368 - dense_3_acc_1: 0.5323 - dense_3_acc_2: 0.2383 - dense_3_acc_3: 0.0600 - dense_3_acc_4: 0.9371 - dense_3_acc_5: 0.0826 - dense_3_acc_6: 0.0130 - dense_3_acc_7: 0.8903 - dense_3_acc_8: 0.1694 - dense_3_acc_9: 0.079 - ETA: 8s - loss: 19.2789 - dense_3_loss: 2.6938 - dense_3_acc: 0.2430 - dense_3_acc_1: 0.5370 - dense_3_acc_2: 0.2416 - dense_3_acc_3: 0.0594 - dense_3_acc_4: 0.9381 - dense_3_acc_5: 0.0915 - dense_3_acc_6: 0.0137 - dense_3_acc_7: 0.8919 - dense_3_acc_8: 0.1734 - dense_3_acc_9: 0.079 - ETA: 7s - loss: 19.2007 - dense_3_loss: 2.6887 - dense_3_acc: 0.2487 - dense_3_acc_1: 0.5403 - dense_3_acc_2: 0.2422 - dense_3_acc_3: 0.0594 - dense_3_acc_4: 0.9390 - dense_3_acc_5: 0.0969 - dense_3_acc_6: 0.0151 - dense_3_acc_7: 0.8935 - dense_3_acc_8: 0.1772 - dense_3_acc_9: 0.080 - ETA: 7s - loss: 19.1175 - dense_3_loss: 2.6860 - dense_3_acc: 0.2559 - dense_3_acc_1: 0.5455 - dense_3_acc_2: 0.2449 - dense_3_acc_3: 0.0593 - dense_3_acc_4: 0.9399 - dense_3_acc_5: 0.1019 - dense_3_acc_6: 0.0155 - dense_3_acc_7: 0.8951 - dense_3_acc_8: 0.1809 - dense_3_acc_9: 0.081 - ETA: 7s - loss: 19.0341 - dense_3_loss: 2.6814 - dense_3_acc: 0.2617 - dense_3_acc_1: 0.5499 - dense_3_acc_2: 0.2476 - dense_3_acc_3: 0.0591 - dense_3_acc_4: 0.9407 - dense_3_acc_5: 0.1093 - dense_3_acc_6: 0.0159 - dense_3_acc_7: 0.8966 - dense_3_acc_8: 0.1843 - dense_3_acc_9: 0.083 - ETA: 6s - loss: 18.9463 - dense_3_loss: 2.6765 - dense_3_acc: 0.2696 - dense_3_acc_1: 0.5549 - dense_3_acc_2: 0.2494 - dense_3_acc_3: 0.0593 - dense_3_acc_4: 0.9415 - dense_3_acc_5: 0.1196 - dense_3_acc_6: 0.0173 - dense_3_acc_7: 0.8980 - dense_3_acc_8: 0.1870 - dense_3_acc_9: 0.084 - ETA: 6s - loss: 18.8602 - dense_3_loss: 2.6709 - dense_3_acc: 0.2769 - dense_3_acc_1: 0.5597 - dense_3_acc_2: 0.2507 - dense_3_acc_3: 0.0597 - dense_3_acc_4: 0.9424 - dense_3_acc_5: 0.1285 - dense_3_acc_6: 0.0182 - dense_3_acc_7: 0.8994 - dense_3_acc_8: 0.1914 - dense_3_acc_9: 0.085 - ETA: 6s - loss: 18.7750 - dense_3_loss: 2.6658 - dense_3_acc: 0.2845 - dense_3_acc_1: 0.5637 - dense_3_acc_2: 0.2516 - dense_3_acc_3: 0.0595 - dense_3_acc_4: 0.9432 - dense_3_acc_5: 0.1374 - dense_3_acc_6: 0.0188 - dense_3_acc_7: 0.9008 - dense_3_acc_8: 0.1948 - dense_3_acc_9: 0.085 - ETA: 6s - loss: 18.6829 - dense_3_loss: 2.6605 - dense_3_acc: 0.2922 - dense_3_acc_1: 0.5681 - dense_3_acc_2: 0.2536 - dense_3_acc_3: 0.0607 - dense_3_acc_4: 0.9439 - dense_3_acc_5: 0.1450 - dense_3_acc_6: 0.0205 - dense_3_acc_7: 0.9022 - dense_3_acc_8: 0.1970 - dense_3_acc_9: 0.086 - ETA: 5s - loss: 18.5959 - dense_3_loss: 2.6547 - dense_3_acc: 0.3005 - dense_3_acc_1: 0.5724 - dense_3_acc_2: 0.2547 - dense_3_acc_3: 0.0612 - dense_3_acc_4: 0.9447 - dense_3_acc_5: 0.1523 - dense_3_acc_6: 0.0216 - dense_3_acc_7: 0.9035 - dense_3_acc_8: 0.2000 - dense_3_acc_9: 0.088 - ETA: 5s - loss: 18.5045 - dense_3_loss: 2.6500 - dense_3_acc: 0.3076 - dense_3_acc_1: 0.5772 - dense_3_acc_2: 0.2570 - dense_3_acc_3: 0.0613 - dense_3_acc_4: 0.9454 - dense_3_acc_5: 0.1607 - dense_3_acc_6: 0.0229 - dense_3_acc_7: 0.9047 - dense_3_acc_8: 0.2030 - dense_3_acc_9: 0.088 - ETA: 5s - loss: 18.4161 - dense_3_loss: 2.6450 - dense_3_acc: 0.3142 - dense_3_acc_1: 0.5810 - dense_3_acc_2: 0.2601 - dense_3_acc_3: 0.0619 - dense_3_acc_4: 0.9461 - dense_3_acc_5: 0.1678 - dense_3_acc_6: 0.0243 - dense_3_acc_7: 0.9060 - dense_3_acc_8: 0.2060 - dense_3_acc_9: 0.089 - ETA: 5s - loss: 18.3244 - dense_3_loss: 2.6403 - dense_3_acc: 0.3210 - dense_3_acc_1: 0.5860 - dense_3_acc_2: 0.2631 - dense_3_acc_3: 0.0622 - dense_3_acc_4: 0.9468 - dense_3_acc_5: 0.1763 - dense_3_acc_6: 0.0247 - dense_3_acc_7: 0.9072 - dense_3_acc_8: 0.2076 - dense_3_acc_9: 0.090 - ETA: 4s - loss: 18.2348 - dense_3_loss: 2.6363 - dense_3_acc: 0.3278 - dense_3_acc_1: 0.5905 - dense_3_acc_2: 0.2649 - dense_3_acc_3: 0.0634 - dense_3_acc_4: 0.9475 - dense_3_acc_5: 0.1844 - dense_3_acc_6: 0.0256 - dense_3_acc_7: 0.9084 - dense_3_acc_8: 0.2109 - dense_3_acc_9: 0.091 - ETA: 4s - loss: 18.1451 - dense_3_loss: 2.6322 - dense_3_acc: 0.3354 - dense_3_acc_1: 0.5951 - dense_3_acc_2: 0.2676 - dense_3_acc_3: 0.0641 - dense_3_acc_4: 0.9481 - dense_3_acc_5: 0.1916 - dense_3_acc_6: 0.0271 - dense_3_acc_7: 0.9095 - dense_3_acc_8: 0.2143 - dense_3_acc_9: 0.091 - ETA: 4s - loss: 18.0570 - dense_3_loss: 2.6277 - dense_3_acc: 0.3428 - dense_3_acc_1: 0.5993 - dense_3_acc_2: 0.2696 - dense_3_acc_3: 0.0647 - dense_3_acc_4: 0.9488 - dense_3_acc_5: 0.1985 - dense_3_acc_6: 0.0298 - dense_3_acc_7: 0.9106 - dense_3_acc_8: 0.2178 - dense_3_acc_9: 0.092 - ETA: 3s - loss: 17.9707 - dense_3_loss: 2.6234 - dense_3_acc: 0.3491 - dense_3_acc_1: 0.6032 - dense_3_acc_2: 0.2723 - dense_3_acc_3: 0.0654 - dense_3_acc_4: 0.9494 - dense_3_acc_5: 0.2048 - dense_3_acc_6: 0.0315 - dense_3_acc_7: 0.9117 - dense_3_acc_8: 0.2195 - dense_3_acc_9: 0.093 - ETA: 3s - loss: 17.8842 - dense_3_loss: 2.6201 - dense_3_acc: 0.3563 - dense_3_acc_1: 0.6073 - dense_3_acc_2: 0.2752 - dense_3_acc_3: 0.0666 - dense_3_acc_4: 0.9500 - dense_3_acc_5: 0.2110 - dense_3_acc_6: 0.0340 - dense_3_acc_7: 0.9128 - dense_3_acc_8: 0.2222 - dense_3_acc_9: 0.093 - ETA: 3s - loss: 17.7978 - dense_3_loss: 2.6159 - dense_3_acc: 0.3631 - dense_3_acc_1: 0.6113 - dense_3_acc_2: 0.2776 - dense_3_acc_3: 0.0682 - dense_3_acc_4: 0.9506 - dense_3_acc_5: 0.2179 - dense_3_acc_6: 0.0356 - dense_3_acc_7: 0.9138 - dense_3_acc_8: 0.2258 - dense_3_acc_9: 0.094 - ETA: 3s - loss: 17.7133 - dense_3_loss: 2.6124 - dense_3_acc: 0.3694 - dense_3_acc_1: 0.6152 - dense_3_acc_2: 0.2804 - dense_3_acc_3: 0.0699 - dense_3_acc_4: 0.9512 - dense_3_acc_5: 0.2244 - dense_3_acc_6: 0.0392 - dense_3_acc_7: 0.9148 - dense_3_acc_8: 0.2278 - dense_3_acc_9: 0.095 - ETA: 3s - loss: 17.6279 - dense_3_loss: 2.6086 - dense_3_acc: 0.3763 - dense_3_acc_1: 0.6192 - dense_3_acc_2: 0.2823 - dense_3_acc_3: 0.0703 - dense_3_acc_4: 0.9517 - dense_3_acc_5: 0.2309 - dense_3_acc_6: 0.0427 - dense_3_acc_7: 0.9158 - dense_3_acc_8: 0.2309 - dense_3_acc_9: 0.095 - ETA: 2s - loss: 17.5457 - dense_3_loss: 2.6051 - dense_3_acc: 0.3821 - dense_3_acc_1: 0.6228 - dense_3_acc_2: 0.2851 - dense_3_acc_3: 0.0713 - dense_3_acc_4: 0.9523 - dense_3_acc_5: 0.2374 - dense_3_acc_6: 0.0470 - dense_3_acc_7: 0.9168 - dense_3_acc_8: 0.2337 - dense_3_acc_9: 0.0967"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - ETA: 2s - loss: 17.4637 - dense_3_loss: 2.6015 - dense_3_acc: 0.3890 - dense_3_acc_1: 0.6265 - dense_3_acc_2: 0.2870 - dense_3_acc_3: 0.0720 - dense_3_acc_4: 0.9528 - dense_3_acc_5: 0.2444 - dense_3_acc_6: 0.0494 - dense_3_acc_7: 0.9177 - dense_3_acc_8: 0.2356 - dense_3_acc_9: 0.096 - ETA: 2s - loss: 17.3818 - dense_3_loss: 2.5967 - dense_3_acc: 0.3944 - dense_3_acc_1: 0.6301 - dense_3_acc_2: 0.2889 - dense_3_acc_3: 0.0737 - dense_3_acc_4: 0.9534 - dense_3_acc_5: 0.2507 - dense_3_acc_6: 0.0524 - dense_3_acc_7: 0.9187 - dense_3_acc_8: 0.2380 - dense_3_acc_9: 0.098 - ETA: 2s - loss: 17.3014 - dense_3_loss: 2.5930 - dense_3_acc: 0.3998 - dense_3_acc_1: 0.6334 - dense_3_acc_2: 0.2911 - dense_3_acc_3: 0.0749 - dense_3_acc_4: 0.9539 - dense_3_acc_5: 0.2564 - dense_3_acc_6: 0.0561 - dense_3_acc_7: 0.9196 - dense_3_acc_8: 0.2396 - dense_3_acc_9: 0.099 - ETA: 1s - loss: 17.2236 - dense_3_loss: 2.5891 - dense_3_acc: 0.4052 - dense_3_acc_1: 0.6366 - dense_3_acc_2: 0.2944 - dense_3_acc_3: 0.0766 - dense_3_acc_4: 0.9544 - dense_3_acc_5: 0.2622 - dense_3_acc_6: 0.0587 - dense_3_acc_7: 0.9204 - dense_3_acc_8: 0.2412 - dense_3_acc_9: 0.100 - ETA: 1s - loss: 17.1478 - dense_3_loss: 2.5855 - dense_3_acc: 0.4110 - dense_3_acc_1: 0.6401 - dense_3_acc_2: 0.2961 - dense_3_acc_3: 0.0773 - dense_3_acc_4: 0.9549 - dense_3_acc_5: 0.2679 - dense_3_acc_6: 0.0605 - dense_3_acc_7: 0.9213 - dense_3_acc_8: 0.2427 - dense_3_acc_9: 0.101 - ETA: 1s - loss: 17.0715 - dense_3_loss: 2.5819 - dense_3_acc: 0.4166 - dense_3_acc_1: 0.6438 - dense_3_acc_2: 0.2986 - dense_3_acc_3: 0.0782 - dense_3_acc_4: 0.9554 - dense_3_acc_5: 0.2744 - dense_3_acc_6: 0.0632 - dense_3_acc_7: 0.9222 - dense_3_acc_8: 0.2445 - dense_3_acc_9: 0.102 - ETA: 1s - loss: 16.9986 - dense_3_loss: 2.5790 - dense_3_acc: 0.4223 - dense_3_acc_1: 0.6471 - dense_3_acc_2: 0.3003 - dense_3_acc_3: 0.0797 - dense_3_acc_4: 0.9559 - dense_3_acc_5: 0.2805 - dense_3_acc_6: 0.0660 - dense_3_acc_7: 0.9230 - dense_3_acc_8: 0.2468 - dense_3_acc_9: 0.102 - ETA: 1s - loss: 16.9234 - dense_3_loss: 2.5750 - dense_3_acc: 0.4278 - dense_3_acc_1: 0.6506 - dense_3_acc_2: 0.3031 - dense_3_acc_3: 0.0812 - dense_3_acc_4: 0.9563 - dense_3_acc_5: 0.2862 - dense_3_acc_6: 0.0696 - dense_3_acc_7: 0.9238 - dense_3_acc_8: 0.2492 - dense_3_acc_9: 0.103 - ETA: 0s - loss: 16.8522 - dense_3_loss: 2.5710 - dense_3_acc: 0.4327 - dense_3_acc_1: 0.6537 - dense_3_acc_2: 0.3047 - dense_3_acc_3: 0.0826 - dense_3_acc_4: 0.9568 - dense_3_acc_5: 0.2910 - dense_3_acc_6: 0.0729 - dense_3_acc_7: 0.9246 - dense_3_acc_8: 0.2517 - dense_3_acc_9: 0.104 - ETA: 0s - loss: 16.7807 - dense_3_loss: 2.5673 - dense_3_acc: 0.4377 - dense_3_acc_1: 0.6566 - dense_3_acc_2: 0.3060 - dense_3_acc_3: 0.0833 - dense_3_acc_4: 0.9572 - dense_3_acc_5: 0.2962 - dense_3_acc_6: 0.0756 - dense_3_acc_7: 0.9254 - dense_3_acc_8: 0.2531 - dense_3_acc_9: 0.105 - ETA: 0s - loss: 16.7109 - dense_3_loss: 2.5656 - dense_3_acc: 0.4433 - dense_3_acc_1: 0.6598 - dense_3_acc_2: 0.3088 - dense_3_acc_3: 0.0840 - dense_3_acc_4: 0.9577 - dense_3_acc_5: 0.3007 - dense_3_acc_6: 0.0779 - dense_3_acc_7: 0.9261 - dense_3_acc_8: 0.2551 - dense_3_acc_9: 0.105 - ETA: 0s - loss: 16.6406 - dense_3_loss: 2.5613 - dense_3_acc: 0.4480 - dense_3_acc_1: 0.6629 - dense_3_acc_2: 0.3110 - dense_3_acc_3: 0.0849 - dense_3_acc_4: 0.9581 - dense_3_acc_5: 0.3053 - dense_3_acc_6: 0.0808 - dense_3_acc_7: 0.9269 - dense_3_acc_8: 0.2580 - dense_3_acc_9: 0.106 - 20s 2ms/step - loss: 16.5706 - dense_3_loss: 2.5572 - dense_3_acc: 0.4520 - dense_3_acc_1: 0.6657 - dense_3_acc_2: 0.3140 - dense_3_acc_3: 0.0854 - dense_3_acc_4: 0.9585 - dense_3_acc_5: 0.3100 - dense_3_acc_6: 0.0827 - dense_3_acc_7: 0.9276 - dense_3_acc_8: 0.2606 - dense_3_acc_9: 0.1079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x40143cf8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('models/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (37, 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ac7ef602615d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstring_to_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhuman_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhuman_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minv_machine_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (37, 30)"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
    "for example in EXAMPLES:\n",
    "    \n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
