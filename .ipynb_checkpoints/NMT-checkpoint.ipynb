{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Machine Translation (NMT) model to translate human readable dates (\"25th of June, 2009\") into machine readable dates (\"2009-06-25\") using ATTENTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "from faker import Faker\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from babel.dates import format_date\n",
    "from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on a dataset of 10000 human readable dates and their equivalent, standardized machine readable dates.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 7877.51it/s]\n"
     ]
    }
   ],
   "source": [
    "m = 10000\n",
    "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9 may 1998', '1998-05-09'),\n",
       " ('10.09.70', '1970-09-10'),\n",
       " ('4/28/90', '1990-04-28'),\n",
       " ('thursday january 26 1995', '1995-01-26'),\n",
       " ('monday march 7 1983', '1983-03-07'),\n",
       " ('sunday may 22 1988', '1988-05-22'),\n",
       " ('tuesday july 8 2008', '2008-07-08'),\n",
       " ('08 sep 1999', '1999-09-08'),\n",
       " ('1 jan 1981', '1981-01-01'),\n",
       " ('monday may 22 1995', '1995-05-22')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dataset`: a list of tuples of (human readable date, machine readable date)\n",
    "- `human_vocab`: a python dictionary mapping all characters used in the human readable dates to an integer-valued index \n",
    "- `machine_vocab`: a python dictionary mapping all characters used in machine readable dates to an integer-valued index. These indices are not necessarily consistent with `human_vocab`. \n",
    "- `inv_machine_vocab`: the inverse dictionary of `machine_vocab`, mapping from indices back to characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (10000, 30)\n",
      "Y.shape: (10000, 10)\n",
      "Xoh.shape: (10000, 30, 37)\n",
      "Yoh.shape: (10000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "Tx = 30   # maximum length of human readable date\n",
    "Ty = 10   # output length ( YYYY-MM-DD )\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `X`: a processed version of the human readable dates in the training set, where each character is replaced by an index mapped to the character via `human_vocab`. Each date is further padded to $T_x$ values with a special character (< pad >). `X.shape = (m, Tx)`\n",
    "- `Y`: a processed version of the machine readable dates in the training set, where each character is replaced by the index it is mapped to in `machine_vocab`. You should have `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`, the \"1\" entry's index is mapped to the character using `human_vocab`. `Xoh.shape = (m, Tx, len(human_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`, the \"1\" entry's index is mapped to the character using `machine_vocab`. `Yoh.shape = (m, Tx, len(machine_vocab))`. Here, `len(machine_vocab) = 11` since there are 11 characters ('-' as well as 0-9). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source date: 9 may 1998\n",
      "Target date: 1998-05-09\n",
      "\n",
      "Source after preprocessing (indices): [12  0 24 13 34  0  4 12 12 11 36 36 36 36 36 36 36 36 36 36 36 36 36 36 36\n",
      " 36 36 36 36 36]\n",
      "Target after preprocessing (indices): [ 2 10 10  9  0  1  6  0  1 10]\n",
      "\n",
      "Source after preprocessing (one-hot): [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Target after preprocessing (one-hot): [[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "index = 0\n",
    "print(\"Source date:\", dataset[index][0])\n",
    "print(\"Target date:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION MECHANISM\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> Neural machine translation with attention</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\"\n",
    "    s_prev = repeator(s_prev)\n",
    "    # use concatenator to concatenate a and s_prev on the last axis\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e.\n",
    "    e = densor1(concat)\n",
    "    # use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies\n",
    "    energies = densor2(e)\n",
    "    # use \"activator\" on \"energies\" to compute the attention weights \"alphas\"\n",
    "    alphas = activator(energies)\n",
    "    # use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell\n",
    "    context = dotor([alphas, a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_a = 32\n",
    "n_s = 64\n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
    "output_layer = Dense(len(machine_vocab), activation=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # the input of the model with a shape (Tx,)\n",
    "    # s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # initialize empty list of outputs\n",
    "    outputs = []\n",
    "       \n",
    "    # define your pre-attention Bi-LSTM. Remember to use return_sequences=True\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # perform one step of the attention mechanism to get back the context vector at step t\n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
    "        \n",
    "        # apply Dense layer to the hidden state output of the post-attention LSTM\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # append \"out\" to the \"outputs\" list\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # create model instance taking three inputs and returning the list of outputs\n",
    "    model = Model([X, s0, c0], outputs)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 30, 37)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 30, 64)        17920       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 30, 64)        0           s0[0][0]                         \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 30, 128)       0           bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[1][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[2][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[3][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[4][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[5][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[6][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[7][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[8][0]            \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   repeat_vector_1[9][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 30, 10)        1290        concatenate_1[0][0]              \n",
      "                                                                   concatenate_1[1][0]              \n",
      "                                                                   concatenate_1[2][0]              \n",
      "                                                                   concatenate_1[3][0]              \n",
      "                                                                   concatenate_1[4][0]              \n",
      "                                                                   concatenate_1[5][0]              \n",
      "                                                                   concatenate_1[6][0]              \n",
      "                                                                   concatenate_1[7][0]              \n",
      "                                                                   concatenate_1[8][0]              \n",
      "                                                                   concatenate_1[9][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 30, 1)         11          dense_1[0][0]                    \n",
      "                                                                   dense_1[1][0]                    \n",
      "                                                                   dense_1[2][0]                    \n",
      "                                                                   dense_1[3][0]                    \n",
      "                                                                   dense_1[4][0]                    \n",
      "                                                                   dense_1[5][0]                    \n",
      "                                                                   dense_1[6][0]                    \n",
      "                                                                   dense_1[7][0]                    \n",
      "                                                                   dense_1[8][0]                    \n",
      "                                                                   dense_1[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Activation)   (None, 30, 1)         0           dense_2[0][0]                    \n",
      "                                                                   dense_2[1][0]                    \n",
      "                                                                   dense_2[2][0]                    \n",
      "                                                                   dense_2[3][0]                    \n",
      "                                                                   dense_2[4][0]                    \n",
      "                                                                   dense_2[5][0]                    \n",
      "                                                                   dense_2[6][0]                    \n",
      "                                                                   dense_2[7][0]                    \n",
      "                                                                   dense_2[8][0]                    \n",
      "                                                                   dense_2[9][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1, 64)         0           attention_weights[0][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[1][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[2][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[3][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[4][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[5][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[6][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[7][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[8][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "                                                                   attention_weights[9][0]          \n",
      "                                                                   bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 64)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 64), (None, 6 33024       dot_1[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_1[1][0]                      \n",
      "                                                                   lstm_1[0][0]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "                                                                   dot_1[2][0]                      \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[1][2]                     \n",
      "                                                                   dot_1[3][0]                      \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[2][2]                     \n",
      "                                                                   dot_1[4][0]                      \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[3][2]                     \n",
      "                                                                   dot_1[5][0]                      \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[4][2]                     \n",
      "                                                                   dot_1[6][0]                      \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[5][2]                     \n",
      "                                                                   dot_1[7][0]                      \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[6][2]                     \n",
      "                                                                   dot_1[8][0]                      \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[7][2]                     \n",
      "                                                                   dot_1[9][0]                      \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[8][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 11)            715         lstm_1[0][0]                     \n",
      "                                                                   lstm_1[1][0]                     \n",
      "                                                                   lstm_1[2][0]                     \n",
      "                                                                   lstm_1[3][0]                     \n",
      "                                                                   lstm_1[4][0]                     \n",
      "                                                                   lstm_1[5][0]                     \n",
      "                                                                   lstm_1[6][0]                     \n",
      "                                                                   lstm_1[7][0]                     \n",
      "                                                                   lstm_1[8][0]                     \n",
      "                                                                   lstm_1[9][0]                     \n",
      "====================================================================================================\n",
      "Total params: 52,960\n",
      "Trainable params: 52,960\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay = 0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10000/10000 [==============================] - 29s - loss: 3.7375 - dense_3_loss_1: 0.0582 - dense_3_loss_2: 0.0509 - dense_3_loss_3: 0.4733 - dense_3_loss_4: 0.7055 - dense_3_loss_5: 0.0066 - dense_3_loss_6: 0.0848 - dense_3_loss_7: 0.7790 - dense_3_loss_8: 0.0067 - dense_3_loss_9: 0.6237 - dense_3_loss_10: 0.9488 - dense_3_acc_1: 0.9814 - dense_3_acc_2: 0.9814 - dense_3_acc_3: 0.8074 - dense_3_acc_4: 0.8012 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9778 - dense_3_acc_7: 0.7628 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.7635 - dense_3_acc_10: 0.6596    \n",
      "Epoch 2/20\n",
      "10000/10000 [==============================] - 28s - loss: 3.4893 - dense_3_loss_1: 0.0571 - dense_3_loss_2: 0.0510 - dense_3_loss_3: 0.4513 - dense_3_loss_4: 0.6405 - dense_3_loss_5: 0.0059 - dense_3_loss_6: 0.0826 - dense_3_loss_7: 0.7288 - dense_3_loss_8: 0.0067 - dense_3_loss_9: 0.5942 - dense_3_loss_10: 0.8712 - dense_3_acc_1: 0.9811 - dense_3_acc_2: 0.9815 - dense_3_acc_3: 0.8160 - dense_3_acc_4: 0.8235 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9786 - dense_3_acc_7: 0.7798 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.7770 - dense_3_acc_10: 0.6874    \n",
      "Epoch 3/20\n",
      "10000/10000 [==============================] - 28s - loss: 3.2627 - dense_3_loss_1: 0.0565 - dense_3_loss_2: 0.0499 - dense_3_loss_3: 0.4312 - dense_3_loss_4: 0.5813 - dense_3_loss_5: 0.0053 - dense_3_loss_6: 0.0784 - dense_3_loss_7: 0.6799 - dense_3_loss_8: 0.0063 - dense_3_loss_9: 0.5759 - dense_3_loss_10: 0.7978 - dense_3_acc_1: 0.9823 - dense_3_acc_2: 0.9812 - dense_3_acc_3: 0.8238 - dense_3_acc_4: 0.8430 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9784 - dense_3_acc_7: 0.7976 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.7827 - dense_3_acc_10: 0.7128    \n",
      "Epoch 4/20\n",
      "10000/10000 [==============================] - 32s - loss: 3.0831 - dense_3_loss_1: 0.0549 - dense_3_loss_2: 0.0489 - dense_3_loss_3: 0.4129 - dense_3_loss_4: 0.5350 - dense_3_loss_5: 0.0047 - dense_3_loss_6: 0.0779 - dense_3_loss_7: 0.6428 - dense_3_loss_8: 0.0061 - dense_3_loss_9: 0.5589 - dense_3_loss_10: 0.7410 - dense_3_acc_1: 0.9814 - dense_3_acc_2: 0.9813 - dense_3_acc_3: 0.8266 - dense_3_acc_4: 0.8593 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9781 - dense_3_acc_7: 0.8116 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.7928 - dense_3_acc_10: 0.7335    \n",
      "Epoch 5/20\n",
      "10000/10000 [==============================] - 33s - loss: 2.8919 - dense_3_loss_1: 0.0537 - dense_3_loss_2: 0.0483 - dense_3_loss_3: 0.3952 - dense_3_loss_4: 0.4891 - dense_3_loss_5: 0.0044 - dense_3_loss_6: 0.0757 - dense_3_loss_7: 0.6011 - dense_3_loss_8: 0.0056 - dense_3_loss_9: 0.5358 - dense_3_loss_10: 0.6831 - dense_3_acc_1: 0.9821 - dense_3_acc_2: 0.9818 - dense_3_acc_3: 0.8334 - dense_3_acc_4: 0.8741 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9779 - dense_3_acc_7: 0.8293 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8036 - dense_3_acc_10: 0.7557    \n",
      "Epoch 6/20\n",
      "10000/10000 [==============================] - 32s - loss: 2.7433 - dense_3_loss_1: 0.0527 - dense_3_loss_2: 0.0480 - dense_3_loss_3: 0.3811 - dense_3_loss_4: 0.4550 - dense_3_loss_5: 0.0040 - dense_3_loss_6: 0.0720 - dense_3_loss_7: 0.5692 - dense_3_loss_8: 0.0053 - dense_3_loss_9: 0.5187 - dense_3_loss_10: 0.6371 - dense_3_acc_1: 0.9819 - dense_3_acc_2: 0.9819 - dense_3_acc_3: 0.8391 - dense_3_acc_4: 0.8825 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9805 - dense_3_acc_7: 0.8411 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8080 - dense_3_acc_10: 0.7729    \n",
      "Epoch 7/20\n",
      "10000/10000 [==============================] - 30s - loss: 2.5987 - dense_3_loss_1: 0.0516 - dense_3_loss_2: 0.0465 - dense_3_loss_3: 0.3677 - dense_3_loss_4: 0.4197 - dense_3_loss_5: 0.0036 - dense_3_loss_6: 0.0704 - dense_3_loss_7: 0.5356 - dense_3_loss_8: 0.0050 - dense_3_loss_9: 0.5024 - dense_3_loss_10: 0.5961 - dense_3_acc_1: 0.9832 - dense_3_acc_2: 0.9824 - dense_3_acc_3: 0.8429 - dense_3_acc_4: 0.8917 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9811 - dense_3_acc_7: 0.8503 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8126 - dense_3_acc_10: 0.7903    \n",
      "Epoch 8/20\n",
      "10000/10000 [==============================] - 28s - loss: 2.4791 - dense_3_loss_1: 0.0509 - dense_3_loss_2: 0.0467 - dense_3_loss_3: 0.3562 - dense_3_loss_4: 0.3952 - dense_3_loss_5: 0.0034 - dense_3_loss_6: 0.0692 - dense_3_loss_7: 0.5080 - dense_3_loss_8: 0.0050 - dense_3_loss_9: 0.4832 - dense_3_loss_10: 0.5612 - dense_3_acc_1: 0.9827 - dense_3_acc_2: 0.9820 - dense_3_acc_3: 0.8452 - dense_3_acc_4: 0.8951 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9801 - dense_3_acc_7: 0.8612 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8227 - dense_3_acc_10: 0.8015    \n",
      "Epoch 9/20\n",
      "10000/10000 [==============================] - 28s - loss: 2.3922 - dense_3_loss_1: 0.0501 - dense_3_loss_2: 0.0458 - dense_3_loss_3: 0.3464 - dense_3_loss_4: 0.3716 - dense_3_loss_5: 0.0031 - dense_3_loss_6: 0.0682 - dense_3_loss_7: 0.4853 - dense_3_loss_8: 0.0047 - dense_3_loss_9: 0.4853 - dense_3_loss_10: 0.5317 - dense_3_acc_1: 0.9837 - dense_3_acc_2: 0.9828 - dense_3_acc_3: 0.8487 - dense_3_acc_4: 0.9020 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9808 - dense_3_acc_7: 0.8660 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8193 - dense_3_acc_10: 0.8167    \n",
      "Epoch 10/20\n",
      "10000/10000 [==============================] - 28s - loss: 2.2742 - dense_3_loss_1: 0.0492 - dense_3_loss_2: 0.0453 - dense_3_loss_3: 0.3378 - dense_3_loss_4: 0.3522 - dense_3_loss_5: 0.0029 - dense_3_loss_6: 0.0662 - dense_3_loss_7: 0.4634 - dense_3_loss_8: 0.0044 - dense_3_loss_9: 0.4538 - dense_3_loss_10: 0.4989 - dense_3_acc_1: 0.9831 - dense_3_acc_2: 0.9825 - dense_3_acc_3: 0.8515 - dense_3_acc_4: 0.9076 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9806 - dense_3_acc_7: 0.8752 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8323 - dense_3_acc_10: 0.8246    \n",
      "Epoch 11/20\n",
      "10000/10000 [==============================] - 29s - loss: 2.1811 - dense_3_loss_1: 0.0490 - dense_3_loss_2: 0.0450 - dense_3_loss_3: 0.3294 - dense_3_loss_4: 0.3359 - dense_3_loss_5: 0.0027 - dense_3_loss_6: 0.0642 - dense_3_loss_7: 0.4396 - dense_3_loss_8: 0.0044 - dense_3_loss_9: 0.4418 - dense_3_loss_10: 0.4690 - dense_3_acc_1: 0.9827 - dense_3_acc_2: 0.9823 - dense_3_acc_3: 0.8543 - dense_3_acc_4: 0.9107 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9817 - dense_3_acc_7: 0.8818 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8418 - dense_3_acc_10: 0.8388    \n",
      "Epoch 12/20\n",
      "10000/10000 [==============================] - 30s - loss: 2.1090 - dense_3_loss_1: 0.0488 - dense_3_loss_2: 0.0453 - dense_3_loss_3: 0.3231 - dense_3_loss_4: 0.3203 - dense_3_loss_5: 0.0026 - dense_3_loss_6: 0.0625 - dense_3_loss_7: 0.4204 - dense_3_loss_8: 0.0041 - dense_3_loss_9: 0.4307 - dense_3_loss_10: 0.4511 - dense_3_acc_1: 0.9835 - dense_3_acc_2: 0.9830 - dense_3_acc_3: 0.8552 - dense_3_acc_4: 0.9120 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9828 - dense_3_acc_7: 0.8894 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8436 - dense_3_acc_10: 0.8413    \n",
      "Epoch 13/20\n",
      "10000/10000 [==============================] - 33s - loss: 2.0279 - dense_3_loss_1: 0.0477 - dense_3_loss_2: 0.0437 - dense_3_loss_3: 0.3135 - dense_3_loss_4: 0.3068 - dense_3_loss_5: 0.0024 - dense_3_loss_6: 0.0610 - dense_3_loss_7: 0.4053 - dense_3_loss_8: 0.0041 - dense_3_loss_9: 0.4159 - dense_3_loss_10: 0.4275 - dense_3_acc_1: 0.9831 - dense_3_acc_2: 0.9831 - dense_3_acc_3: 0.8558 - dense_3_acc_4: 0.9185 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9825 - dense_3_acc_7: 0.8924 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8526 - dense_3_acc_10: 0.8557    \n",
      "Epoch 14/20\n",
      "10000/10000 [==============================] - 31s - loss: 1.9586 - dense_3_loss_1: 0.0474 - dense_3_loss_2: 0.0439 - dense_3_loss_3: 0.3082 - dense_3_loss_4: 0.2949 - dense_3_loss_5: 0.0022 - dense_3_loss_6: 0.0604 - dense_3_loss_7: 0.3899 - dense_3_loss_8: 0.0038 - dense_3_loss_9: 0.3997 - dense_3_loss_10: 0.4082 - dense_3_acc_1: 0.9837 - dense_3_acc_2: 0.9828 - dense_3_acc_3: 0.8580 - dense_3_acc_4: 0.9181 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9822 - dense_3_acc_7: 0.8974 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8541 - dense_3_acc_10: 0.8626    \n",
      "Epoch 15/20\n",
      "10000/10000 [==============================] - 30s - loss: 1.9038 - dense_3_loss_1: 0.0468 - dense_3_loss_2: 0.0435 - dense_3_loss_3: 0.3023 - dense_3_loss_4: 0.2845 - dense_3_loss_5: 0.0021 - dense_3_loss_6: 0.0583 - dense_3_loss_7: 0.3773 - dense_3_loss_8: 0.0037 - dense_3_loss_9: 0.3917 - dense_3_loss_10: 0.3936 - dense_3_acc_1: 0.9833 - dense_3_acc_2: 0.9832 - dense_3_acc_3: 0.8597 - dense_3_acc_4: 0.9202 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9833 - dense_3_acc_7: 0.9001 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8581 - dense_3_acc_10: 0.8676    \n",
      "Epoch 16/20\n",
      "10000/10000 [==============================] - 32s - loss: 1.8420 - dense_3_loss_1: 0.0459 - dense_3_loss_2: 0.0423 - dense_3_loss_3: 0.2975 - dense_3_loss_4: 0.2746 - dense_3_loss_5: 0.0020 - dense_3_loss_6: 0.0574 - dense_3_loss_7: 0.3662 - dense_3_loss_8: 0.0035 - dense_3_loss_9: 0.3790 - dense_3_loss_10: 0.3737 - dense_3_acc_1: 0.9836 - dense_3_acc_2: 0.9843 - dense_3_acc_3: 0.8597 - dense_3_acc_4: 0.9237 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9836 - dense_3_acc_7: 0.9035 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8642 - dense_3_acc_10: 0.8774    \n",
      "Epoch 17/20\n",
      "10000/10000 [==============================] - 33s - loss: 1.7820 - dense_3_loss_1: 0.0457 - dense_3_loss_2: 0.0421 - dense_3_loss_3: 0.2917 - dense_3_loss_4: 0.2650 - dense_3_loss_5: 0.0019 - dense_3_loss_6: 0.0559 - dense_3_loss_7: 0.3527 - dense_3_loss_8: 0.0033 - dense_3_loss_9: 0.3664 - dense_3_loss_10: 0.3573 - dense_3_acc_1: 0.9835 - dense_3_acc_2: 0.9834 - dense_3_acc_3: 0.8634 - dense_3_acc_4: 0.9270 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9833 - dense_3_acc_7: 0.9054 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8688 - dense_3_acc_10: 0.8812    \n",
      "Epoch 18/20\n",
      "10000/10000 [==============================] - 32s - loss: 1.7369 - dense_3_loss_1: 0.0449 - dense_3_loss_2: 0.0417 - dense_3_loss_3: 0.2877 - dense_3_loss_4: 0.2557 - dense_3_loss_5: 0.0018 - dense_3_loss_6: 0.0548 - dense_3_loss_7: 0.3427 - dense_3_loss_8: 0.0032 - dense_3_loss_9: 0.3569 - dense_3_loss_10: 0.3475 - dense_3_acc_1: 0.9838 - dense_3_acc_2: 0.9839 - dense_3_acc_3: 0.8615 - dense_3_acc_4: 0.9293 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9842 - dense_3_acc_7: 0.9094 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8740 - dense_3_acc_10: 0.8865    \n",
      "Epoch 19/20\n",
      "10000/10000 [==============================] - 28s - loss: 1.6921 - dense_3_loss_1: 0.0450 - dense_3_loss_2: 0.0418 - dense_3_loss_3: 0.2838 - dense_3_loss_4: 0.2490 - dense_3_loss_5: 0.0017 - dense_3_loss_6: 0.0538 - dense_3_loss_7: 0.3330 - dense_3_loss_8: 0.0031 - dense_3_loss_9: 0.3485 - dense_3_loss_10: 0.3323 - dense_3_acc_1: 0.9843 - dense_3_acc_2: 0.9842 - dense_3_acc_3: 0.8637 - dense_3_acc_4: 0.9310 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9841 - dense_3_acc_7: 0.9102 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8762 - dense_3_acc_10: 0.8911    \n",
      "Epoch 20/20\n",
      "10000/10000 [==============================] - 28s - loss: 1.6397 - dense_3_loss_1: 0.0444 - dense_3_loss_2: 0.0413 - dense_3_loss_3: 0.2800 - dense_3_loss_4: 0.2425 - dense_3_loss_5: 0.0017 - dense_3_loss_6: 0.0525 - dense_3_loss_7: 0.3251 - dense_3_loss_8: 0.0029 - dense_3_loss_9: 0.3328 - dense_3_loss_10: 0.3165 - dense_3_acc_1: 0.9845 - dense_3_acc_2: 0.9836 - dense_3_acc_3: 0.8620 - dense_3_acc_4: 0.9308 - dense_3_acc_5: 1.0000 - dense_3_acc_6: 0.9835 - dense_3_acc_7: 0.9124 - dense_3_acc_8: 1.0000 - dense_3_acc_9: 0.8826 - dense_3_acc_10: 0.8975    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f01ce36c0b8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pretrained model\n",
    "# model.load_weights('models/model.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pp(example):\n",
    "    source = string_to_int(example, Tx, human_vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n",
    "    prediction = model.predict([source, s0, c0])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
    "    \n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: 1 March 2001\n",
      "output: 2000-03-01\n"
     ]
    }
   ],
   "source": [
    "pp(\"1 March 2001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting attention values from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f01c82716d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAGsCAYAAAD9ro91AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIpI4JWsREZHEKVmLiIgk7v8DjjQqvbu0tTMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f01c81f6630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
